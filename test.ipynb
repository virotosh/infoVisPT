{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3023be1-6e98-4c62-97ec-73b30f7d8c16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd67e5cd-f4c9-44f6-99b5-fc2def1f6002",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfb4c0ef-ace7-4c77-b2d3-ef627879dabc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "paths = [str(x) for x in Path('data').glob('**/*.txt')]\n",
    "\n",
    "tokenizer = BertWordPieceTokenizer(\n",
    "        clean_text=True,\n",
    "        handle_chinese_chars=False,\n",
    "        strip_accents=False,\n",
    "        lowercase=True\n",
    ")\n",
    "tokenizer.train(files=paths, vocab_size=30_000, min_frequency=2,\n",
    "                    limit_alphabet=1000, wordpieces_prefix='##',\n",
    "                    special_tokens=['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61a3a6b9-54dd-477e-98d0-f2f698742ad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model/vocab.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "tokenizer.save_model('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5055e8d-f4bd-4eb9-865f-d0effe2d82e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39057e74-f7b4-4ffc-a4a5-8a129ac5f61a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [2, 2697, 447, 3], 'token_type_ids': [0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[CLS] hey there [SEP]'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer('hey there')\n",
    "print(tokens)\n",
    "# {'input_ids': [2, 21694, 16, 2287, 2009, 1991, 35, 3], \n",
    "# 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}\n",
    "\n",
    "tokenizer.decode(tokens['input_ids']) \n",
    "# '[CLS] hello, how are you? [SEP]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773cc618-2131-422a-af27-c0d787051e32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6483c2dd-1cac-4842-ae57-d20927e5f7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0373cce-c2d4-4413-9aad-7340f79729b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    This class loads and preprocesses the given text data\n",
    "    \"\"\"\n",
    "    def __init__(self, paths, tokenizer):\n",
    "        \"\"\"\n",
    "        This function initialises the object. It takes the given paths and tokeniser.\n",
    "        \"\"\"\n",
    "        # the last file might not have 10000 samples, which makes it difficult to get the total length of the ds\n",
    "        self.paths = paths[:len(paths)-1]\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = self.read_file(self.paths[0])\n",
    "        self.current_file = 1\n",
    "        self.remaining = len(self.data)\n",
    "        self.encodings = self.get_encodings(self.data)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        returns the lenght of the ds\n",
    "        \"\"\"\n",
    "        return 10000*len(self.paths)\n",
    "    \n",
    "    def read_file(self, path):\n",
    "        \"\"\"\n",
    "        reads a given file\n",
    "        \"\"\"\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            lines = f.read().split('\\n')\n",
    "        return lines\n",
    "\n",
    "    def get_encodings(self, lines_all):\n",
    "        \"\"\"\n",
    "        Creates encodings for a given text input\n",
    "        \"\"\"\n",
    "        # tokenise all text \n",
    "        batch = self.tokenizer(lines_all, max_length=512, padding='max_length', truncation=True)\n",
    "\n",
    "        # Ground Truth\n",
    "        labels = torch.tensor(batch['input_ids'])\n",
    "        # Attention Masks\n",
    "        mask = torch.tensor(batch['attention_mask'])\n",
    "\n",
    "        # Input to be masked\n",
    "        input_ids = labels.detach().clone()\n",
    "        rand = torch.rand(input_ids.shape)\n",
    "\n",
    "        # with a probability of 15%, mask a given word, leave out CLS, SEP and PAD\n",
    "        mask_arr = (rand < .15) * (input_ids != 0) * (input_ids != 2) * (input_ids != 3)\n",
    "        # assign token 4 (=MASK)\n",
    "        input_ids[mask_arr] = 4\n",
    "        \n",
    "        return {'input_ids':input_ids, 'attention_mask':mask, 'labels':labels}\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        \"\"\"\n",
    "        returns item i\n",
    "        Note: do not use shuffling for this dataset\n",
    "        \"\"\"\n",
    "        # if we have looked at all items in the file - take next\n",
    "        if self.remaining == 0:\n",
    "            self.data = self.read_file(self.paths[self.current_file])\n",
    "            self.current_file += 1\n",
    "            self.remaining = len(self.data)\n",
    "            self.encodings = self.get_encodings(self.data)\n",
    "        \n",
    "        # if we are at the end of the dataset, start over again\n",
    "        if self.current_file == len(self.paths):\n",
    "            self.current_file = 0\n",
    "                 \n",
    "        self.remaining -= 1    \n",
    "        return {key: tensor[i%10000] for key, tensor in self.encodings.items()}  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7d0cd2a-ac3d-4fba-86fb-4c1cf4015718",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(paths = [str(x) for x in Path('data/').glob('**/*.txt')][:], tokenizer=tokenizer)\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=8)\n",
    "\n",
    "test_dataset = Dataset(paths = [str(x) for x in Path('data/').glob('**/*.txt')][300:], tokenizer=tokenizer)\n",
    "valid_loader = torch.utils.data.DataLoader(test_dataset, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db42eb99-583f-44cf-bbcd-65159740cc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertForMaskedLM, DistilBertConfig\n",
    "\n",
    "config = DistilBertConfig(\n",
    "    vocab_size=30000,\n",
    "    max_position_embeddings=514\n",
    ")\n",
    "model = DistilBertForMaskedLM(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc5aeba3-647a-4b7d-8b05-e9d7a990f241",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## PRETRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b618a30d-1c09-45b7-9c9b-025607355899",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838de0cf-dd3d-4b07-a948-0c7e736cc09d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05dde35-d2b6-49ab-b9bb-9fbaf30eee6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                | 0/737500 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "epochs = 10\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    \n",
    "    # set model to training mode\n",
    "    model.train()\n",
    "    losses = []\n",
    "    \n",
    "    # iterate over dataset\n",
    "    for batch in loop:\n",
    "        optim.zero_grad()\n",
    "        \n",
    "        # copy input to device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # predict\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        \n",
    "        # update weights\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        \n",
    "        optim.step()\n",
    "        \n",
    "        # output current loss\n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "    print(\"Mean Training Loss\", np.mean(losses))\n",
    "    losses = []\n",
    "    loop = tqdm(test_loader, leave=True)\n",
    "    \n",
    "    # set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # iterate over dataset\n",
    "    for batch in loop:\n",
    "        # copy input to device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # predict\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        \n",
    "        # update weights\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # output current loss\n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "        losses.append(loss.item())\n",
    "    print(\"Mean Test Loss\", np.mean(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a89159d-668c-4271-8800-6a2f189c6687",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
